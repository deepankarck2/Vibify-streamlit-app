{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3020c80",
   "metadata": {},
   "source": [
    "# Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e6a9ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras import layers, models, losses, optimizers,callbacks\n",
    "\n",
    "from keras.layers import Dense,LSTM\n",
    "from keras.utils import pad_sequences\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f890099",
   "metadata": {},
   "source": [
    "# load preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e137d0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(158353, 6)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('preprocessed_data.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22651b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_lyrics'] = df['clean_lyrics'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd0188c",
   "metadata": {},
   "source": [
    "# Create X and y feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6859345f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=0.1)\n",
    "\n",
    "X = df['clean_lyrics']\n",
    "\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2316e3a",
   "metadata": {},
   "source": [
    "# Vectorize X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5975eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15835, 300)\n"
     ]
    }
   ],
   "source": [
    "# Limiting our tokenizers vocab size\n",
    "max_words = 10000\n",
    " \n",
    "    \n",
    "# create the tokenizer\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "\n",
    "\n",
    "# Fit the tokenizer\n",
    "tokenizer.fit_on_texts(X)\n",
    "\n",
    "\n",
    "# Create the sequences for each sentence, basically turning each word into its index position\n",
    "sequences = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "\n",
    "index_word = tokenizer.index_word\n",
    "\n",
    "\n",
    "# # Limiting our sequencer to only include 300 words\n",
    "max_length = 300\n",
    "\n",
    "\n",
    "# # Convert the sequences to all be the same length of 300\n",
    "X = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52ce2b7",
   "metadata": {},
   "source": [
    "# Split train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "772d6c12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12668, 300), (12668,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=45)\n",
    "X_train.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc713a46",
   "metadata": {},
   "source": [
    "# Building LSTM nueral net "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78548331",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 300, 32)           320000    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 300, 50)           16600     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 50)                20200     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 356,851\n",
      "Trainable params: 356,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dip2l\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# This creates the Neural Network\n",
    "model = Sequential() \n",
    "\n",
    "# This embedding layer basically will automatically create the word2vec vectors based on your text data.\n",
    "model.add( Embedding(max_words, 32, input_length=max_length) ) \n",
    "\n",
    "model.add(LSTM(50,return_sequences=True,dropout =0.2))\n",
    "model.add(LSTM(50,dropout =0.2))\n",
    "model.add(Dense(1,kernel_initializer='normal',  activation='linear'))\n",
    "optimizer = optimizers.Adam(lr=0.003)\n",
    "model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['mse']) \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6e9581",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0db4cca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "507/507 [==============================] - 123s 233ms/step - loss: 0.0653 - mse: 0.0653 - val_loss: 0.0615 - val_mse: 0.0615\n"
     ]
    }
   ],
   "source": [
    "callback = callbacks.EarlyStopping(monitor='val_mse',patience = 2,restore_best_weights=True)\n",
    "hist = model.fit(X_train, y_train, \n",
    "                 validation_split=0.2,\n",
    "                 epochs=1, batch_size=20,callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90c4be23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keras.engine.sequential.Sequential"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3cdd989b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store\n",
    "model.save('LSTM_Valence_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72cf72e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = keras.models.load_model('LSTM_Valence_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f24131e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'load_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [50]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#load\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m loaded_model \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLSTM_Valence_model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'load_model'"
     ]
    }
   ],
   "source": [
    "#load\n",
    "loaded_model = tf.load_model('LSTM_Valence_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6cd0bc71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject'>\n"
     ]
    }
   ],
   "source": [
    "print(type(loaded_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "439b36cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Model.predict() missing 1 required positional argument: 'x'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mloaded_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "\u001b[1;31mTypeError\u001b[0m: Model.predict() missing 1 required positional argument: 'x'"
     ]
    }
   ],
   "source": [
    "loaded_model.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "01035ada",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_UserObject' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [52]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mloaded_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m(X)\n",
      "\u001b[1;31mAttributeError\u001b[0m: '_UserObject' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "loaded_model.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1b3a59",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bf0bf6c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_UserObject' object has no attribute 'evaluate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m mse\u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m(X_test, y_test, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest mse with stacked LSTM:\u001b[39m\u001b[38;5;124m'\u001b[39m, mse)\n",
      "\u001b[1;31mAttributeError\u001b[0m: '_UserObject' object has no attribute 'evaluate'"
     ]
    }
   ],
   "source": [
    "mse= model.evaluate(X_test, y_test, verbose=0)[1]\n",
    "\n",
    "print('Test mse with stacked LSTM:', mse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "2baaea0077e5265e426630973480ae39020f208133c2b899d28f4fc53e2dc4a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
